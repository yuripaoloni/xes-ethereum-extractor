{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses the data generated with [Ethtx](https://github.com/EthTx) using their [beta data warehouses](https://tokenflow.live/blog/edw-open). The data refers to the transactions of the [LANDProxy](https://etherscan.io/address/0xf87e31492faf9a91b02ee0deaad50d51d56d5d4d) contract and the subcalls of each transaction.\n",
    "\n",
    "The goal is to produce a dataframe for each unique `FUNCTION_NAME` contained in the data. On such dataframes, all the transactions and subcalls for the `FUNCTION_NAME` are present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "492107\n",
      "Index(['LOAD_ID', 'CHAIN_ID', 'BLOCK', 'TIMESTAMP', 'TX_HASH', 'CALL_ID',\n",
      "       'CALL_TYPE', 'FROM_ADDRESS', 'FROM_NAME', 'TO_ADDRESS', 'TO_NAME',\n",
      "       'FUNCTION_SIGNATURE', 'FUNCTION_NAME', 'VALUE', 'ARGUMENTS',\n",
      "       'RAW_ARGUMENTS', 'OUTPUTS', 'RAW_OUTPUTS', 'GAS_USED', 'ERROR',\n",
      "       'STATUS', 'ORDER_INDEX', 'DECODING_STATUS', 'STORAGE_ADDRESS'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import glob, os\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "path = r'../data/LAND_decoded_calls'\n",
    "all_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "df = pd.concat((pd.read_csv(f,  sep=\",\", engine=\"python\", escapechar='\\\\')\n",
    "               for f in all_files))\n",
    "\n",
    "# df = pd.read_csv(r'../data/LAND_decoded_calls\\LAND_decoded_calls_0_0_0.csv', sep=\",\", engine=\"python\", escapechar='\\\\')\n",
    "\n",
    "print(df.shape[0])\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since transactions can happen inside the same block, they will have the same timestamp. To give a time order to the records, we sort them using `TIMESTAMP` and `ORDER_INDEX` fields and add incrementally 1 second to records with same timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['ERROR'] == '\\\\N'] # remove error transactions\n",
    "\n",
    "df = df.sort_values(by=[\"TIMESTAMP\", \"ORDER_INDEX\"]) # sort by TIMESTAMP and ORDER_INDEX\n",
    "df['TIMESTAMP'] = pd.to_datetime(df['TIMESTAMP']) # convert TIMESTAMP from object to datetime\n",
    "\n",
    "# iterate through the df to change equal timestamps\n",
    "last_timestamp = \"\";\n",
    "counter = 1;\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if(row[\"TIMESTAMP\"] == last_timestamp):\n",
    "        counter = counter + 1;\n",
    "        new_timestamp = pd.to_datetime(row[\"TIMESTAMP\"] + pd.to_timedelta(counter, unit='s'))\n",
    "    else:\n",
    "        last_timestamp = row[\"TIMESTAMP\"]\n",
    "        counter = 1\n",
    "        new_timestamp = pd.to_datetime(row[\"TIMESTAMP\"] + pd.to_timedelta(counter, unit='s'))  \n",
    "    \n",
    "    df.at[index,'TIMESTAMP'] = new_timestamp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create FUNCTION_NAME dataframes:** create a new column where the `CALL_ID` is equal to `\"\\\\N\"`, then group by `TX_HASH` and map the `TX_HASH` to the first value of the newly created column. (As there can only be one `CALL_ID==\"\\\\N\"` per `TX_HASH`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "# create differents dfs for each FUNCTION_NAME when CALL_ID == \\\\N\n",
    "df[\"NEW_HASH_GROUP\"] = (df.CALL_ID == \"\\\\N\") * df.FUNCTION_NAME\n",
    "df[\"GROUP\"] = df.TX_HASH.map(df.groupby(\"TX_HASH\").NEW_HASH_GROUP.first())\n",
    "\n",
    "dfs = [f for _, f in df.groupby([\"GROUP\"])]\n",
    "\n",
    "print(len(dfs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we iterate through the generated list of dataframes (`dfs`) and create a `.xes` file for each dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "exporting log, completed traces :: 100%|██████████| 327/327 [00:00<00:00, 2895.08it/s]\n",
      "exporting log, completed traces :: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "exporting log, completed traces :: 100%|██████████| 19/19 [00:00<00:00, 4703.80it/s]\n",
      "exporting log, completed traces :: 100%|██████████| 287/287 [00:00<00:00, 5622.97it/s]\n",
      "exporting log, completed traces :: 100%|██████████| 5990/5990 [00:01<00:00, 5826.84it/s]\n",
      "exporting log, completed traces :: 100%|██████████| 5/5 [00:00<00:00, 5000.36it/s]\n",
      "exporting log, completed traces :: 100%|██████████| 51/51 [00:00<00:00, 5100.98it/s]\n",
      "exporting log, completed traces :: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "exporting log, completed traces :: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "exporting log, completed traces :: 100%|██████████| 4/4 [00:00<00:00, 1000.55it/s]\n",
      "exporting log, completed traces :: 100%|██████████| 4895/4895 [00:10<00:00, 484.18it/s]\n",
      "exporting log, completed traces :: 100%|██████████| 6/6 [00:00<00:00, 2999.86it/s]\n",
      "exporting log, completed traces :: 100%|██████████| 14/14 [00:00<00:00, 4603.70it/s]\n",
      "exporting log, completed traces :: 100%|██████████| 3/3 [00:00<00:00, 3003.08it/s]\n",
      "exporting log, completed traces :: 100%|██████████| 41/41 [00:00<00:00, 4538.57it/s]\n",
      "exporting log, completed traces :: 100%|██████████| 2/2 [00:00<00:00, 996.98it/s]\n",
      "exporting log, completed traces :: 100%|██████████| 91/91 [00:00<00:00, 2603.20it/s]\n",
      "exporting log, completed traces :: 100%|██████████| 1348/1348 [00:01<00:00, 1201.46it/s]\n",
      "exporting log, completed traces :: 100%|██████████| 10905/10905 [00:01<00:00, 5521.53it/s]\n",
      "exporting log, completed traces :: 100%|██████████| 4/4 [00:00<00:00, 1333.96it/s]\n",
      "exporting log, completed traces :: 100%|██████████| 210/210 [00:00<00:00, 3750.24it/s]\n",
      "exporting log, completed traces :: 100%|██████████| 3990/3990 [00:00<00:00, 5443.38it/s]\n",
      "exporting log, completed traces :: 100%|██████████| 4649/4649 [00:00<00:00, 5489.05it/s]\n",
      "exporting log, completed traces :: 100%|██████████| 3870/3870 [00:00<00:00, 5657.89it/s]\n",
      "exporting log, completed traces :: 100%|██████████| 14/14 [00:00<00:00, 1555.83it/s]\n",
      "exporting log, completed traces :: 100%|██████████| 13/13 [00:00<00:00, 2166.48it/s]\n",
      "exporting log, completed traces :: 100%|██████████| 5088/5088 [00:22<00:00, 226.06it/s]\n",
      "exporting log, completed traces :: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "exporting log, completed traces :: 100%|██████████| 9/9 [00:00<00:00, 1800.47it/s]\n",
      "exporting log, completed traces :: 100%|██████████| 13204/13204 [00:02<00:00, 5755.78it/s]\n",
      "exporting log, completed traces :: 100%|██████████| 163/163 [00:00<00:00, 5822.45it/s]\n",
      "exporting log, completed traces :: 100%|██████████| 2/2 [00:00<00:00, 2001.10it/s]\n"
     ]
    }
   ],
   "source": [
    "from pm4py.objects.log.util import dataframe_utils\n",
    "from pm4py.objects.conversion.log import converter as log_converter\n",
    "from pm4py.objects.log.exporter.xes import exporter as xes_exporter\n",
    "\n",
    "for df in dfs:\n",
    "    df = dataframe_utils.convert_timestamp_columns_in_df(\n",
    "        df)\n",
    "\n",
    "    function_name = df['GROUP'].iloc[0] # FUNCTION_NAME\n",
    "\n",
    "    # remove unnecessary fields\n",
    "    df.drop([\"LOAD_ID\", \"CHAIN_ID\", \"VALUE\", \"RAW_ARGUMENTS\", \"RAW_OUTPUTS\", \"GAS_USED\", \"DECODING_STATUS\", \"STORAGE_ADDRESS\", \"ERROR\", \"STATUS\", \"NEW_HASH_GROUP\", \"GROUP\" ], axis=1, inplace=True)\n",
    "\n",
    "    df.dropna(inplace=True) # drop null values (in case any)\n",
    "\n",
    "    # create columns: from -> case:concept:name, inputFunctionName -> concept:name, timeStamp -> time:timestamp, from -> org:resource\n",
    "    df[\"org:resource\"] = df[\"FROM_ADDRESS\"]\n",
    "    df[\"case:concept:name\"] = df[\"TX_HASH\"]\n",
    "    df[\"time:timestamp\"] = df[\"TIMESTAMP\"]\n",
    "    df[\"concept:name\"] = df[\"FUNCTION_NAME\"]\n",
    "\n",
    "    # specify that the field identifying the case identifier attribute is the field with name 'case:concept:name'\n",
    "    parameters = {\n",
    "        log_converter.Variants.TO_EVENT_LOG.value.Parameters.CASE_ID_KEY: 'case:concept:name'}\n",
    "    log = log_converter.apply(df, parameters=parameters,\n",
    "                            variant=log_converter.Variants.TO_EVENT_LOG)\n",
    "\n",
    "    xes_exporter.apply(\n",
    "        log, f\"../data/internal_trace/{function_name}_land_proxy.xes\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1527373efda72fa2755c1bf8e28aa37d44fcfaa765722a53896873e9ccde169f"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
