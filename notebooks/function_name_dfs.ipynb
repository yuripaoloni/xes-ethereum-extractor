{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses the data generated with [Ethtx](https://github.com/EthTx) using their [beta data warehouses](https://tokenflow.live/blog/edw-open). The data refers to the transactions of the [LANDProxy](https://etherscan.io/address/0xf87e31492faf9a91b02ee0deaad50d51d56d5d4d) contract and the subcalls of each transaction.\n",
    "\n",
    "The goal is to produce a dataframe for each unique `FUNCTION_NAME` contained in the data. On such dataframes, all the transactions and subcalls for the `FUNCTION_NAME` are present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9724\n",
      "Index(['LOAD_ID', 'CHAIN_ID', 'BLOCK', 'TIMESTAMP', 'TX_HASH', 'CALL_ID',\n",
      "       'CALL_TYPE', 'FROM_ADDRESS', 'FROM_NAME', 'TO_ADDRESS', 'TO_NAME',\n",
      "       'FUNCTION_SIGNATURE', 'FUNCTION_NAME', 'VALUE', 'ARGUMENTS',\n",
      "       'RAW_ARGUMENTS', 'OUTPUTS', 'RAW_OUTPUTS', 'GAS_USED', 'ERROR',\n",
      "       'STATUS', 'ORDER_INDEX', 'DECODING_STATUS', 'STORAGE_ADDRESS'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import glob, os\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# path = r'../data/LAND_decoded_calls'\n",
    "# all_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "# df = pd.concat((pd.read_csv(f,  sep=\",\", engine=\"python\", escapechar='\\\\')\n",
    "#                for f in all_files))\n",
    "\n",
    "df = pd.read_csv(r'../data/LAND_decoded_calls\\LAND_decoded_calls_0_0_0.csv', sep=\",\", engine=\"python\", escapechar='\\\\')\n",
    "\n",
    "print(df.shape[0])\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since transactions can happen inside the same block, they will have the same timestamp. To give a time order to the records, we sort them using `TIMESTAMP` and `ORDER_INDEX` fields and add incrementally 1 second to records with same timestamp.\n",
    "\n",
    "Moreover, we add the `ORIGIN_ADDRESS` field to the transaction record and to the related subcalls, and the `FUNCTION_NAME` of the subcalls is prefixed with the `FROM_NAME` (e.g. `LAND.approve`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['ERROR'] == '\\\\N'] # remove errored transactions\n",
    "\n",
    "df = df.sort_values(by=[\"TIMESTAMP\", \"ORDER_INDEX\"]) # sort by TIMESTAMP and ORDER_INDEX\n",
    "df['TIMESTAMP'] = pd.to_datetime(df['TIMESTAMP']) # convert TIMESTAMP from object to datetime\n",
    "\n",
    "# iterate through the df to change equal timestamps, function_name and add origin_address\n",
    "last_timestamp = \"\";\n",
    "counter = 1;\n",
    "user_address = \"\";\n",
    "\n",
    "for row in df.itertuples():\n",
    "    if(row.TIMESTAMP == last_timestamp):\n",
    "        counter = counter + 1;\n",
    "        new_timestamp = pd.to_datetime(row.TIMESTAMP + pd.to_timedelta(counter, unit='s'))\n",
    "    else:\n",
    "        last_timestamp = row.TIMESTAMP\n",
    "        counter = 1\n",
    "        new_timestamp = pd.to_datetime(row.TIMESTAMP + pd.to_timedelta(counter, unit='s'))  \n",
    "\n",
    "    if(row.CALL_ID == '\\\\N'):\n",
    "        user_address = row.FROM_ADDRESS\n",
    "    else:\n",
    "        df.loc[row.Index, 'FUNCTION_NAME'] = f'{row.FROM_NAME}.{row.FUNCTION_NAME}'\n",
    "    \n",
    "    df.loc[row.Index, 'TIMESTAMP'] = new_timestamp\n",
    "    df.loc[row.Index, 'ORIGIN_ADDRESS'] = user_address "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create FUNCTION_NAME dataframes:** create a new column where the `CALL_ID` is equal to `\"\\\\N\"`, then group by `TX_HASH` and map the `TX_HASH` to the first value of the newly created column. (As there can only be one `CALL_ID==\"\\\\N\"` per `TX_HASH`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "# create differents dfs for each FUNCTION_NAME when CALL_ID == \\\\N\n",
    "df[\"NEW_HASH_GROUP\"] = (df.CALL_ID == \"\\\\N\") * df.FUNCTION_NAME\n",
    "df[\"GROUP\"] = df.TX_HASH.map(df.groupby(\"TX_HASH\").NEW_HASH_GROUP.first())\n",
    "\n",
    "dfs = [f for _, f in df.groupby([\"GROUP\"])]\n",
    "\n",
    "print(len(dfs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the records with `CALL_ID == \\\\N` since we are interested only on the internal calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['CALL_ID'] != '\\\\N']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we iterate through the generated list of dataframes (`dfs`) and create a `.xes` file for each dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "exporting log, completed traces :: 100%|██████████| 12/12 [00:00<00:00, 3001.29it/s]\n",
      "exporting log, completed traces :: 100%|██████████| 8/8 [00:00<00:00, 3998.86it/s]\n",
      "exporting log, completed traces :: 100%|██████████| 7/7 [00:00<00:00, 3500.67it/s]\n",
      "exporting log, completed traces :: 100%|██████████| 146/146 [00:00<00:00, 404.36it/s]\n",
      "exporting log, completed traces :: 100%|██████████| 2/2 [00:00<00:00, 1000.31it/s]\n",
      "exporting log, completed traces :: 100%|██████████| 3/3 [00:00<00:00, 1505.31it/s]\n",
      "exporting log, completed traces :: 100%|██████████| 46/46 [00:00<00:00, 920.03it/s]\n",
      "exporting log, completed traces :: 100%|██████████| 337/337 [00:00<00:00, 5026.76it/s]\n",
      "exporting log, completed traces :: 100%|██████████| 5/5 [00:00<00:00, 4997.98it/s]\n",
      "exporting log, completed traces :: 100%|██████████| 107/107 [00:00<00:00, 4855.04it/s]\n",
      "exporting log, completed traces :: 100%|██████████| 86/86 [00:00<00:00, 4081.31it/s]\n",
      "exporting log, completed traces :: 100%|██████████| 119/119 [00:00<00:00, 4951.66it/s]\n",
      "exporting log, completed traces :: 100%|██████████| 76/76 [00:00<00:00, 226.17it/s]\n",
      "exporting log, completed traces :: 100%|██████████| 423/423 [00:00<00:00, 5035.84it/s]\n",
      "exporting log, completed traces :: 100%|██████████| 3/3 [00:00<00:00, 3017.48it/s]\n"
     ]
    }
   ],
   "source": [
    "from pm4py.objects.log.util import dataframe_utils\n",
    "from pm4py.objects.conversion.log import converter as log_converter\n",
    "from pm4py.objects.log.exporter.xes import exporter as xes_exporter\n",
    "\n",
    "for df in dfs:\n",
    "    df = dataframe_utils.convert_timestamp_columns_in_df(\n",
    "        df)\n",
    "\n",
    "    function_name = df['GROUP'].iloc[0] # FUNCTION_NAME\n",
    "\n",
    "    # remove unnecessary fields\n",
    "    df.drop([\"LOAD_ID\", \"CHAIN_ID\", \"VALUE\", \"RAW_ARGUMENTS\", \"RAW_OUTPUTS\", \"GAS_USED\", \"DECODING_STATUS\", \"STORAGE_ADDRESS\", \"ERROR\", \"STATUS\", \"NEW_HASH_GROUP\", \"GROUP\" ], axis=1, inplace=True)\n",
    "\n",
    "    df.dropna(inplace=True) # drop null values (in case any)\n",
    "\n",
    "    # create columns: from -> case:concept:name, inputFunctionName -> concept:name, timeStamp -> time:timestamp, from -> org:resource\n",
    "    df[\"org:resource\"] = df[\"FROM_ADDRESS\"]\n",
    "    df[\"case:concept:name\"] = df[\"TX_HASH\"]\n",
    "    df[\"time:timestamp\"] = df[\"TIMESTAMP\"]\n",
    "    df[\"concept:name\"] = df[\"FUNCTION_NAME\"]\n",
    "\n",
    "    # specify that the field identifying the case identifier attribute is the field with name 'case:concept:name'\n",
    "    parameters = {\n",
    "        log_converter.Variants.TO_EVENT_LOG.value.Parameters.CASE_ID_KEY: 'case:concept:name'}\n",
    "    log = log_converter.apply(df, parameters=parameters,\n",
    "                            variant=log_converter.Variants.TO_EVENT_LOG)\n",
    "\n",
    "    xes_exporter.apply(\n",
    "        log, f\"../data/internal_trace/{function_name}_land_proxy.xes\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1527373efda72fa2755c1bf8e28aa37d44fcfaa765722a53896873e9ccde169f"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
