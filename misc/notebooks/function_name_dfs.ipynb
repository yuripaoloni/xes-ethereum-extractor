{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses the data generated with [Ethtx](https://github.com/EthTx) using their [beta data warehouses](https://tokenflow.live/blog/edw-open). The data refers to the transactions of the [LANDProxy](https://etherscan.io/address/0xf87e31492faf9a91b02ee0deaad50d51d56d5d4d) contract and the subcalls of each transaction.\n",
    "\n",
    "The goal is to produce a dataframe for each unique `FUNCTION_NAME` contained in the data. On such dataframes, all the transactions and subcalls for the `FUNCTION_NAME` are present.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13946\n",
      "Index(['LOAD_ID', 'CHAIN_ID', 'BLOCK', 'TIMESTAMP', 'TX_HASH', 'CALL_ID',\n",
      "       'CALL_TYPE', 'FROM_ADDRESS', 'FROM_NAME', 'TO_ADDRESS', 'TO_NAME',\n",
      "       'FUNCTION_SIGNATURE', 'FUNCTION_NAME', 'VALUE', 'ARGUMENTS',\n",
      "       'RAW_ARGUMENTS', 'OUTPUTS', 'RAW_OUTPUTS', 'GAS_USED', 'ERROR',\n",
      "       'STATUS', 'ORDER_INDEX', 'DECODING_STATUS', 'STORAGE_ADDRESS'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# path = r'../data/LAND_decoded_calls'\n",
    "# all_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "\n",
    "all_files = [\n",
    "    r'../data/cryptokitties_decoded_calls/KittyCore.csv',\n",
    "            #  r'../data/cryptokitties_decoded_calls/SaleClockAuction.csv'\n",
    "             ]\n",
    "\n",
    "df = pd.concat((pd.read_csv(f,  sep=\",\", engine=\"python\", escapechar='\\\\')\n",
    "               for f in all_files))\n",
    "\n",
    "# df = pd.read_csv(r'../data/LAND_decoded_calls\\LAND_decoded_calls_0_0_0.csv', sep=\",\", engine=\"python\", escapechar='\\\\')\n",
    "\n",
    "print(df.shape[0])\n",
    "print(df.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since transactions can happen inside the same block, they will have the same timestamp. To give a time order to the records, we sort them using `TIMESTAMP` and `ORDER_INDEX` fields and add incrementally 1 second to records with same timestamp.\n",
    "\n",
    "Moreover, we add the `ORIGIN_ADDRESS` field to the transaction record and to the related subcalls.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by TIMESTAMP and ORDER_INDEX\n",
    "df = df.sort_values(by=[\"TIMESTAMP\", \"ORDER_INDEX\"])\n",
    "df.reset_index(drop=True, inplace=True) # needed after sorting\n",
    "\n",
    "user_address = \"\"\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if(pd.isnull(row[\"FUNCTION_NAME\"])): \n",
    "        df.at[index, 'FUNCTION_NAME'] = \"fallback\"\n",
    "    if(pd.isnull(row[\"CALL_ID\"])):\n",
    "        user_address = row[\"FROM_ADDRESS\"]\n",
    "\n",
    "    df.at[index, 'ORIGIN_ADDRESS'] = user_address"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create FUNCTION_NAME dataframes:** create a new column where the `CALL_ID` is equal to `\"\\\\N\"`, then group by `TX_HASH` and map the `TX_HASH` to the first value of the newly created column. (As there can only be one `CALL_ID==\"\\\\N\"` per `TX_HASH`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "# create differents dfs for each FUNCTION_NAME when CALL_ID == \\\\N\n",
    "df[\"NEW_HASH_GROUP\"] = (pd.isnull(row[\"CALL_ID\"])) * df.FUNCTION_NAME\n",
    "df[\"GROUP\"] = df.TX_HASH.map(df.groupby(\"TX_HASH\").NEW_HASH_GROUP.first())\n",
    "\n",
    "dfs = [f for _, f in df.groupby([\"GROUP\"])]\n",
    "\n",
    "print(len(dfs))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the `FROM_NAME` value as prefix in the `FUNCTION_NAME` of subcalls record. This makes possible to distinguish top-level transactions from subcalls.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in dfs:\n",
    "    for index, row in df.iterrows():\n",
    "        if(pd.isnull(row[\"CALL_ID\"])):\n",
    "            df.at[index,\n",
    "                  'FUNCTION_NAME'] = f\"{row['FROM_NAME']}_{row['FUNCTION_NAME']}\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we iterate through the generated list of dataframes (`dfs`) and create a `.xes` file for each dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "approve\n",
      "0 internal transactions\n",
      "bidOnSiringAuction\n",
      "1718 internal transactions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "exporting log, completed traces :: 100%|██████████| 352/352 [00:00<00:00, 2314.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "breedWithAuto\n",
      "0 internal transactions\n",
      "createGen0Auction\n",
      "603 internal transactions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "exporting log, completed traces :: 100%|██████████| 201/201 [00:00<00:00, 3858.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "createSaleAuction\n",
      "2873 internal transactions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "exporting log, completed traces :: 100%|██████████| 1437/1437 [00:00<00:00, 5513.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "createSiringAuction\n",
      "926 internal transactions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "exporting log, completed traces :: 100%|██████████| 463/463 [00:00<00:00, 5403.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "giveBirth\n",
      "0 internal transactions\n",
      "transfer\n",
      "0 internal transactions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from pm4py.objects.log.util import dataframe_utils\n",
    "from pm4py.objects.conversion.log import converter as log_converter\n",
    "from pm4py.objects.log.exporter.xes import exporter as xes_exporter\n",
    "\n",
    "for df in dfs:\n",
    "    df = dataframe_utils.convert_timestamp_columns_in_df(\n",
    "        df)\n",
    "\n",
    "    function_name = df['GROUP'].iloc[0]  # FUNCTION_NAME\n",
    "    print(function_name)\n",
    "\n",
    "    # remove the records with `CALL_ID == NaN` since we are interested only on the internal transactions\n",
    "    df.dropna(subset=[\"CALL_ID\"], inplace=True)\n",
    "\n",
    "    print(f\"{len(df)} internal transactions\")\n",
    "    if(len(df) == 0): continue\n",
    "\n",
    "    # remove unnecessary fields\n",
    "    df.drop([\"LOAD_ID\", \"CHAIN_ID\", \"VALUE\", \"RAW_ARGUMENTS\", \"RAW_OUTPUTS\", \"GAS_USED\", \"DECODING_STATUS\",\n",
    "            \"STORAGE_ADDRESS\", \"ERROR\", \"STATUS\", \"NEW_HASH_GROUP\", \"GROUP\"], axis=1, inplace=True)\n",
    "\n",
    "    # create columns: from -> case:concept:name, inputFunctionName -> concept:name, timeStamp -> time:timestamp\n",
    "    df[\"case:concept:name\"] = df[\"TX_HASH\"]\n",
    "    df[\"time:timestamp\"] = df[\"TIMESTAMP\"]\n",
    "    df[\"concept:name\"] = df[\"FUNCTION_NAME\"]\n",
    "\n",
    "    # specify that the field identifying the case identifier attribute is the field with name 'case:concept:name'\n",
    "    parameters = {\n",
    "        log_converter.Variants.TO_EVENT_LOG.value.Parameters.CASE_ID_KEY: 'case:concept:name'}\n",
    "    log = log_converter.apply(df, parameters=parameters,\n",
    "                              variant=log_converter.Variants.TO_EVENT_LOG)\n",
    "\n",
    "    # xes_exporter.apply(\n",
    "    #     log, f\"../data/internal_trace/{function_name}_land_proxy.xes\")\n",
    "    xes_exporter.apply(\n",
    "        log, f\"../data/logs/cryptokitties_subcalls/{function_name}_cryptokitties.xes\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1527373efda72fa2755c1bf8e28aa37d44fcfaa765722a53896873e9ccde169f"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
