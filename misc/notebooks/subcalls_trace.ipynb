{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses the data generated with [Ethtx](https://github.com/EthTx) using their [beta data warehouses](https://tokenflow.live/blog/edw-open). The data refers to the transactions of the [LANDProxy](https://etherscan.io/address/0xf87e31492faf9a91b02ee0deaad50d51d56d5d4d) contract and the subcalls of each transaction.\n",
    "\n",
    "The goal is to produce a XES log containing the combination of public transactions and internal transactions that can be used by an algorithm that deals with subprocesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10737\n",
      "Index(['LOAD_ID', 'CHAIN_ID', 'BLOCK', 'TIMESTAMP', 'TX_HASH', 'CALL_ID',\n",
      "       'CALL_TYPE', 'FROM_ADDRESS', 'FROM_NAME', 'TO_ADDRESS', 'TO_NAME',\n",
      "       'FUNCTION_SIGNATURE', 'FUNCTION_NAME', 'VALUE', 'ARGUMENTS',\n",
      "       'RAW_ARGUMENTS', 'OUTPUTS', 'RAW_OUTPUTS', 'GAS_USED', 'ERROR',\n",
      "       'STATUS', 'ORDER_INDEX', 'DECODING_STATUS', 'STORAGE_ADDRESS'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import glob, os\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# path = r'../data/LAND_decoded_calls'\n",
    "# all_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "\n",
    "all_files = [r'../data/LAND_decoded_calls\\LAND_decoded_calls_1_6_0.csv', \n",
    "# r'../data/LAND_decoded_calls\\LAND_decoded_calls_0_1_0.csv', \n",
    "#  r'../data/LAND_decoded_calls\\LAND_decoded_calls_2_4_0.csv', \n",
    "#  r'../data/LAND_decoded_calls\\LAND_decoded_calls_3_3_0.csv', \n",
    "#  r'../data/LAND_decoded_calls\\LAND_decoded_calls_0_4_0.csv'\n",
    "]\n",
    "\n",
    "df = pd.concat((pd.read_csv(f,  sep=\",\", engine=\"python\", escapechar='\\\\')\n",
    "               for f in all_files))\n",
    "\n",
    "\n",
    "print(df.shape[0])\n",
    "print(df.columns)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To give an order to the records, we sort them using `TIMESTAMP` and `ORDER_INDEX` fields and then reset the indexes for the iteration with `iterrows()`. \n",
    "\n",
    "To make the dataset work with algorithms that deals with subprocesses we need to make some changes to the records. The `FUNCTION_NAME` of the subcalls is prefixed with `{function_name}_{row['FROM_NAME']` (e.g. `approve_LAND.approve`) to highlights the sub-process name (`function_name`) and the smart contract that is calling it (`row['FROM_NAME]`). Moreover, we need to add new columns with the name of the \"top-level\" transaction method (e.g. `destroy`, `transferFrom`, `createEstate`) to the subcalls in order to group them in subprocesses. These columns have as values the hash of the top-level transaction. \n",
    "\n",
    "The `id` column is added to the transaction record and to the related subcalls to group them in traces. This column contains the address of the user invoking the \"top-level\" transaction. The value added in the `id` column can be changed based on what we want to use to create traces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = df.sort_values(by=[\"TIMESTAMP\", \"ORDER_INDEX\"]) # sort by TIMESTAMP and ORDER_INDEX\n",
    "df.reset_index(drop=True, inplace=True) # needed after sorting\n",
    "\n",
    "user_address = \"\"\n",
    "function_name = \"\"\n",
    "calls_length = 0\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if(row[\"CALL_ID\"] == \"\\\\N\"):\n",
    "        # update_columns(calls_length=calls_length, function_name=function_name)\n",
    "\n",
    "        calls_length = 0\n",
    "        user_address = row[\"FROM_ADDRESS\"]\n",
    "        function_name = row[\"FUNCTION_NAME\"]\n",
    "        df.at[index, 'FUNCTION_NAME'] = f\"{row['TO_NAME']}{'.'}{row['FUNCTION_NAME']}\"\n",
    "    else:\n",
    "        calls_length += 1\n",
    "        df.at[index, 'FUNCTION_NAME'] = f\"{function_name}_{row['FROM_NAME']}{'.'}{row['FUNCTION_NAME']}\"\n",
    "        df.at[index, function_name] = row[\"TX_HASH\"]\n",
    "\n",
    "    df.at[index, 'id'] = user_address "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the `.xes` log with the `id` column as trace key and remove \"nan\" attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "exporting log, completed traces :: 100%|██████████| 840/840 [00:00<00:00, 4467.43it/s]\n"
     ]
    }
   ],
   "source": [
    "from pm4py.objects.log.util import dataframe_utils\n",
    "from pm4py.objects.conversion.log import converter as log_converter\n",
    "from pm4py.objects.log.exporter.xes import exporter as xes_exporter\n",
    "\n",
    "df = dataframe_utils.convert_timestamp_columns_in_df(\n",
    "    df)\n",
    "\n",
    "# create XES standard columns\n",
    "df[\"case:concept:name\"] = df[\"id\"]\n",
    "df[\"time:timestamp\"] = df[\"TIMESTAMP\"]\n",
    "df[\"concept:name\"] = df[\"FUNCTION_NAME\"]\n",
    "\n",
    "# remove unnecessary fields\n",
    "df.drop(['LOAD_ID', 'CHAIN_ID', 'BLOCK', 'TIMESTAMP', 'TX_HASH', 'CALL_ID',\n",
    "       'CALL_TYPE', 'FROM_ADDRESS', 'FROM_NAME', 'TO_ADDRESS', 'TO_NAME',\n",
    "       'FUNCTION_SIGNATURE', 'FUNCTION_NAME', 'VALUE', 'ARGUMENTS',\n",
    "       'RAW_ARGUMENTS', 'OUTPUTS', 'RAW_OUTPUTS', 'GAS_USED', 'ERROR',\n",
    "       'STATUS', 'ORDER_INDEX', 'DECODING_STATUS', 'STORAGE_ADDRESS'], axis=1, inplace=True)\n",
    "\n",
    "# specify that the field identifying the case identifier attribute is the field with name 'case:concept:name'\n",
    "parameters = {\n",
    "    log_converter.Variants.TO_EVENT_LOG.value.Parameters.CASE_ID_KEY: 'case:concept:name'}\n",
    "log = log_converter.apply(df, parameters=parameters,\n",
    "                          variant=log_converter.Variants.TO_EVENT_LOG)\n",
    "\n",
    "events = 0\n",
    "\n",
    "# remove \"nan\" attributes from events\n",
    "for t in log:\n",
    "    events += len(t)\n",
    "    for i, e in enumerate(t):\n",
    "        t[i] = {k: v for k, v in e.items() if pd.Series(v).notna().all()}\n",
    "\n",
    "print(f\"Traces: {len(log)}\")\n",
    "print(f\"Events: {events}\")\n",
    "\n",
    "events_type = len(df.columns) - 4 # 4 are 'id', 'case:concept:name', 'time:timestamp', 'concept:name'\n",
    "\n",
    "print(f\"Events type: {events_type}\")\n",
    "print(f\"Duplication ratio: {events / events_type}\")\n",
    "\n",
    "xes_exporter.apply(\n",
    "    log, \"../data/logs/combination_test.xes\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "55c0836f695e51edfda3bde7b361e82ed259093ded1687d2ec4fcce610de72cc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
